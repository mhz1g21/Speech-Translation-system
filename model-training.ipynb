{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True)\n",
    "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map = {}\n",
    "characters = ['<SPACE>', \"'\"] + [chr(i) for i in range(97, 123)]  # Add <SPACE> and ' along with lowercase alphabets\n",
    "\n",
    "for i, char in enumerate(characters):\n",
    "    char_map[char] = i\n",
    "\n",
    "print(char_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class TextProcessing:\n",
    "    def __init__(self, char_map):\n",
    "        self.char_map = char_map\n",
    "        self.index_map = {i: char for char, i in char_map.items()}\n",
    "        self.index_map[0] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        # Encode text to number sequence using list comprehension\n",
    "        return [self.char_map.get(c, self.char_map['<SPACE>']) for c in text]\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        # Decode number sequence to text using list comprehension\n",
    "        return ''.join(self.index_map[i] for i in labels).replace('<SPACE>', ' ')\n",
    "\n",
    "train_transfroms = nn.Sequential(\n",
    "        torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "        torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "        torchaudio.transforms.TimeMasking(time_mask_param=100))\n",
    "    \n",
    "trusted_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "text_transform = TextProcessing(char_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    # Initialize lists to hold spectrograms, labels, input lengths, and label lengths\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    # Loop through each data item\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        # Apply the appropriate transformation to the waveform based on the data type\n",
    "        if data_type == 'train':\n",
    "            spec = train_transfroms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            spec = trusted_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        # Append the transformed spectrogram to the list\n",
    "        spectrograms.append(spec)\n",
    "        # Convert the utterance to integers and append to the labels list\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        # Append the length of the spectrogram and label to their respective lists\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "    # Pad the sequences of spectrograms and labels so they all have the same length\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    # Return the spectrograms, labels, and their lengths\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network def:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STTModel(nn.Module):\n",
    "    def __init__(self, cnn_layer, rnn_layers, rnn_dim,n_class, n_feats, stride=2, dropout=0,device='cuda:0'):\n",
    "        super(STTModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        # cnn for extracting heirachal features\n",
    "        self.cnn = nn.Conv2d(1,32,3, stride=stride, padding=3//2)\n",
    "        # residual cnn for extracting heirachal features and reducing the sequence length\n",
    "        self.resnetlayer = []\n",
    "        for i in range(cnn_layer):\n",
    "            self.resnetlayer.append({\n",
    "                'cnn1':nn.Conv2d(32, 32, 3, 1, 1).to(device),\n",
    "                'cnn2':nn.Conv2d(32, 32, 3, 1, 1).to(device),\n",
    "                'ln1':nn.LayerNorm(n_feats).to(device),\n",
    "                'ln2':nn.LayerNorm(n_feats).to(device),\n",
    "                'actF':nn.ReLU(inplace=True).to(device),\n",
    "                'dp':nn.Dropout(dropout).to(device)}\n",
    "            )\n",
    "        self.fully_connected = nn.Linear(32*n_feats, rnn_dim)\n",
    "        #LSTM for sequence modeling\n",
    "        self.LSTM = []\n",
    "        for i in range(rnn_layers):\n",
    "            self.LSTM.append({\n",
    "                'lstm':nn.LSTM(input_size=rnn_dim if i==0 else rnn_dim*2, hidden_size=rnn_dim,\n",
    "                        num_layers=1, batch_first=i==0, bidirectional=True).to(device),\n",
    "                'ln':nn.LayerNorm(rnn_dim if i==0 else rnn_dim*2).to(device),\n",
    "                'dp':nn.Dropout(dropout).to(device),\n",
    "                'actF':nn.ReLU(inplace=True).to(device)}\n",
    "            )\n",
    "        #classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        #\n",
    "        for res in self.resnetlayer:\n",
    "            x = self.res_forward(x,res)\n",
    "        #\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.fully_connected(x)\n",
    "        #\n",
    "        for lstm in self.LSTM:\n",
    "            x = self.lstm_foward(x,lstm)\n",
    "        #\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def res_forward(self, x,layer):\n",
    "        residual = x\n",
    "        out = layer['cnn1'](x)#cnn1\n",
    "        out = out.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        out = layer['ln1'](out)#layernorm1\n",
    "        out = out.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n",
    "        out = layer['actF'](out)#relu\n",
    "        out = layer['dp'](out)#dropout\n",
    "        out = layer['cnn2'](out)#cnn2\n",
    "        out = out.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        out = layer['ln2'](out)#layernorm2\n",
    "        out= out.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n",
    "        out += residual\n",
    "        out = layer['actF'](out)#relu\n",
    "        out = layer['dp'](out)#dropout\n",
    "        return out\n",
    "    \n",
    "    def lstm_foward(self ,x ,layer):\n",
    "        x = layer['ln'](x)#layernorm\n",
    "        x = layer['actF'](x)#relu\n",
    "        x, _ = layer['lstm'](x)#lstm\n",
    "        x = layer['dp'](x)#dropout\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    # Use list comprehension to generate targets\n",
    "    targets = [text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()) for i in range(len(arg_maxes))]\n",
    "    # Use list comprehension to generate decodes\n",
    "    decodes = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = [index.item() for j, index in enumerate(args) if index != blank_label and not (collapse_repeated and j != 0 and index == args[j -1])]\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "def process_strings(target, predicted, delm=' ', remove_space=False):\n",
    "    target = target.lower()\n",
    "    predicted = predicted.lower()\n",
    "    if remove_space:\n",
    "        target = target.replace(' ', '')\n",
    "        predicted = predicted.replace(' ', '')\n",
    "    target = target.split(delm)\n",
    "    predicted = predicted.split(delm)\n",
    "    return target, predicted\n",
    "\n",
    "def levenshtein_errors(target, predicted, remove_space=False, delm=' '):\n",
    "    target, predicted = process_strings(target, predicted, delm, remove_space)\n",
    "    distance = levenshtein_distance(target, predicted)\n",
    "    return distance, len(target)\n",
    "\n",
    "def cer(target, predicted, remove_space=False):\n",
    "    distance, target_length = levenshtein_errors(target, predicted, remove_space)\n",
    "    return distance / target_length\n",
    "\n",
    "def wer(target, predicted, delm=' '):\n",
    "    distance, target_length = levenshtein_errors(target, predicted, False, delm)\n",
    "    return distance / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as functional\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def calculate_loss_and_backpropagate(model, spectrograms, labels, input_lengths, label_lengths, criterion, optimiser, scheduler):\n",
    "    optimiser.zero_grad()\n",
    "    output = model(spectrograms)\n",
    "    output = functional.log_softmax(output, dim=2)\n",
    "    output = output.transpose(0, 1)\n",
    "    loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    scheduler.step()\n",
    "    return loss, output\n",
    "\n",
    "def calculate_errors(decoded_preds, decoded_targets):\n",
    "    errors = [cer(decoded_targets[j], decoded_preds[j]) for j in range(len(decoded_preds))]\n",
    "    return errors\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimiser, scheduler, epoch):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    train_loss = 0\n",
    "    train_errors = []\n",
    "    \n",
    "    ten_percent_of_data = len(train_loader) // 10\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        if batch_idx > ten_percent_of_data:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(spectrograms)}/{data_len} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "            break\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        loss, output = calculate_loss_and_backpropagate(model, spectrograms, labels, input_lengths, label_lengths, criterion, optimiser, scheduler)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        decoded_preds, decoded_targets = outputDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "        train_errors.extend(calculate_errors(decoded_preds, decoded_targets))\n",
    "\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print(decoded_preds)\n",
    "            print(decoded_targets)\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(spectrograms)}/{data_len} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_error = sum(train_errors) / len(train_errors)\n",
    "    print(f\"Train Epoch: {epoch}\\tAverage Loss: {train_loss:.6f}\\tAverage Error: {train_error:.6f}\")\n",
    "\n",
    "    return train_loss, train_error\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    test_errors = []  # Track test errors\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            if i == np.random.randint(1, 10):\n",
    "                break\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            output = model(spectrograms)\n",
    "            output = functional.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1)\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "            decoded_preds, decoded_targets = outputDecoder(output.transpose(0,1), labels, label_lengths)\n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "    \n",
    "            test_error = sum(test_cer) / len(test_cer)  # Calculate test error\n",
    "            test_errors.append(test_error)  # Append test error to the list\n",
    "    \n",
    "    print(f\"Test Epoch: {epoch}\\tAverage Loss: {test_loss:.6f}\\tAverage Error: {sum(test_errors)/len(test_errors):.6f}\")\n",
    "    epochs = [i for i in range(1, len(test_errors) + 1)] \n",
    "    return test_loss, sum(test_errors)/len(test_errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "def initialize_data_loader(dataset, batch_size, shuffle, process_type, **kwargs):\n",
    "    return data.DataLoader(dataset=dataset,\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=shuffle,\n",
    "                           collate_fn=lambda x: data_processing(x, process_type),\n",
    "                           **kwargs)\n",
    "\n",
    "def initialize_model(params):\n",
    "    return STTModel(params['cnnLayers'], \n",
    "                    params['lstmLayers'], \n",
    "                    params['rnnDim'], \n",
    "                    params['nClass'], \n",
    "                    params['nFeats'], \n",
    "                    params['stride'], \n",
    "                    params['dropout'])\n",
    "\n",
    "learningRate = 5e-4\n",
    "batchSize = 20\n",
    "epochs = 100\n",
    "\n",
    "params = {\n",
    "    \"cnnLayers\" : 3,\n",
    "    \"lstmLayers\" : 5,\n",
    "    \"rnnDim\" : 256,\n",
    "    \"nClass\" : 29,\n",
    "    \"nFeats\" : 128,\n",
    "    \"stride\" : 2,\n",
    "    \"dropout\" : 0\n",
    "}\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda:0')\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True}\n",
    "\n",
    "trainLoader = initialize_data_loader(train_dataset, batchSize, True, 'train', **kwargs)\n",
    "testLoader = initialize_data_loader(test_dataset, batchSize, False, 'test', **kwargs)\n",
    "\n",
    "model = initialize_model(params)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learningRate)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learningRate, steps_per_epoch=len(trainLoader), epochs=epochs)\n",
    "criterion = nn.CTCLoss(blank=28).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_history = []\n",
    "loss_history = []\n",
    "train_error_history = []\n",
    "train_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    x,y = train(model, device, trainLoader, criterion, optimizer, scheduler, epoch)\n",
    "    train_error_history.append(y)\n",
    "    train_loss_history.append(x)\n",
    "    a,b = test(model, device, testLoader, criterion, epoch)\n",
    "    error_history.append(b)\n",
    "    loss_history.append(a)\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(model.state_dict(), f\"modelv2_epoch_{epoch}.pt\")\n",
    "        plt.plot(np.arange(1,len(error_history)+1,1), error_history)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Error')\n",
    "        plt.title(f\"Error at epoch: {epoch}\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(np.arange(1,len(loss_history)+1,1), loss_history)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title(f\"loss as epoch: {epoch}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()  # run garbage collector\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder(output, blank_label = 28, collapse_repeated = True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./LibriSpeech/train-clean-100/19/227/19-227-0000.flac\"\n",
    "waveform, sample_rate = torchaudio.load(file)\n",
    "print(waveform.shape)\n",
    "inputTransforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "input = inputTransforms(waveform).squeeze(0).transpose(0, 1)\n",
    "print(input.shape)\n",
    "input = input.view(128,-1)\n",
    "input = input.unsqueeze(0).unsqueeze(0)\n",
    "print(input.shape)\n",
    "with torch.no_grad():\n",
    "    output = model(input.to(device))\n",
    "print(output)\n",
    "print(output.shape)\n",
    "text = Decoder(output)\n",
    "print(text)\n",
    "#output = model(spectrogram.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model.load_state_dict(torch.load(f\"modelv2_epoch_90.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Load the dataset\n",
    "eval_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True)\n",
    "\n",
    "# Define the input transforms\n",
    "inputTransforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128)\n",
    "cers = []\n",
    "\n",
    "# Evaluate the model on the dataset\n",
    "for waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id in eval_dataset:\n",
    "    # Apply the input transforms\n",
    "    input = inputTransforms(waveform).squeeze(0).transpose(0, 1)\n",
    "    input = input.view(128, -1)\n",
    "    input = input.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Pass the input through the model\n",
    "    with torch.no_grad():\n",
    "        output = model(input.to(device))\n",
    "    #print(output)\n",
    "    # Decode the output\n",
    "    decoded_output = Decoder(output)\n",
    "\n",
    "    # Print the shape of the input and the decoded output\n",
    "    #print(f'Input shape: {input.shape}')\n",
    "    #print(f'Decoded output: {decoded_output}')\n",
    "    # Calculate the CER\n",
    "    #print(decoded_output)\n",
    "    charErrorRate = cer(decoded_output[0], utterance)\n",
    "    cers.append(charErrorRate)\n",
    "    #print(charErrorRate*100)\n",
    "\n",
    "# Convert the CERs to percentages\n",
    "cers_percent = [cer * 100 for cer in cers]\n",
    "\n",
    "# Plot a scatter plot of the CERs\n",
    "plt.scatter(range(len(cers_percent)), cers_percent)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('CER (%)')\n",
    "plt.title('Character Error Rate for each output')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
